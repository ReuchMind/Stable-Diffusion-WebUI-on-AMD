ğŸ§  Stable Diffusion WebUI on AMD (7900 XTX) â€” Step-by-Step Guide
This guide explains how to get Stable Diffusion WebUI (AUTOMATIC1111 version) working on an AMD GPU, specifically tested with the AMD Radeon RX 7900 XTX on Windows. It covers installation, common errors, and how to avoid CPU fallback.

âœ… Requirements
Windows 10/11

AMD GPU with DirectML support (we used Radeon RX 7900 XTX)

Python 3.10.6

Git

Basic knowledge of command line

ğŸ› ï¸ 1. Install Python 3.10.6 (not higher) + GIT
Download from:

https://gitforwindows.org/

https://www.python.org/downloads/release/python-3106/

During installation of Python:

âœ… Check "Add Python to PATH"
âœ… Choose custom install â†’ make sure pip is included

ğŸ—ƒï¸ 2. Set Up Your Folder Structure
Open File Explorer and create the following folder:

makefile
KopiÃ«ren
Bewerken
C:\stable-diffusion\webui
Inside the webui folder, right-click the address bar and select "Open in Terminal" or type cmd and press Enter.

ğŸ“… 3. Clone AMD-Compatible WebUI
bash
KopiÃ«ren
Bewerken
git clone https://github.com/lshqqytiger/stable-diffusion-webui-amdgpu.git
cd stable-diffusion-webui-amdgpu
ğŸ§¹ 4. Modify webui-user.bat
Open webui-user.bat and set the following:

bash
KopiÃ«ren
Bewerken
set PYTHON=
set GIT=
set VENV_DIR=
set COMMANDLINE_ARGS=--skip-torch-cuda-test --no-half --use-directml
call webui.bat
ğŸ’¡ What these flags do:
--skip-torch-cuda-test â€“ Avoids checking for CUDA (we're on AMD, CUDA is NVIDIA)

--no-half â€“ Prevents half-precision issues

--use-directml â€“ Enables DirectML backend (AMD-compatible)

â–¶ï¸ 5. Run the WebUI
Double-click or run webui-user.bat.
If successful, you will see:

nginx
KopiÃ«ren
Bewerken
Running on local URL: http://127.0.0.1:7860
Open it in your browser. ğŸ‰

ğŸ§± 6. Common Errors + Fixes
âŒ AttributeError: module 'torch' has no attribute 'dml'
Fix:

bash
KopiÃ«ren
Bewerken
pip install torch-directml
âŒ AttributeError: module 'torch' has no attribute 'storage'
Fix (optional, only if persistent):

bash
KopiÃ«ren
Bewerken
pip uninstall torch
pip install torch==1.12
âŒ RuntimeError: Input type (float) and bias type (struct c10::Half) should be the same
Fix:
Already fixed with:

bash
KopiÃ«ren
Bewerken
--no-half
âŒ Failed to install ZLUDA: 'NoneType' object is not subscriptable
Fix:
ZLUDA is experimental â€” skip it and stick with:

bash
KopiÃ«ren
Bewerken
--use-directml
ğŸ”¥ Proof of GPU Acceleration
Use Task Manager â†’ Performance tab â†’ GPU

Under â€œCompute 0â€ and â€œVRAM usageâ€, you should see activity while generating.

If not, double-check the flags and that torch-directml is installed.

ğŸ™Œ Final Tips
Donâ€™t update Python beyond 3.10.6

Always use the amdgpu fork, not the regular WebUI repo

Use VRAM-friendly flags if needed:

bash
KopiÃ«ren
Bewerken
--lowvram --opt-sub-quad-attention
â¤ï¸ Credits
This README was crafted after real-world testing with a 7900 XTX.
Created by ReuchMind.

Got it working? Share it! Fork the repo, add your tweaks, and help the next! ğŸ’ª
